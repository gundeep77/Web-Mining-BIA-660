{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center>Assignment 1</center></h1>\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Please read the problem description carefully\n",
    "- Make sure to complete all requirement (shown as bullets) . In general, it would be much easier if you complete the requirements in the order as shown in the problem description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze the frequency of words in a string (3 points)\n",
    " - Define a function which does the following:\n",
    "     * has a string as an input\n",
    "     * splits the string into a list of tokens by space. \n",
    "         - e.g., \"it's a hello world!!!\" will be split into two tokens [\"it's\", \"a\",\"hello\",\"world!!!\"]   \n",
    "     * if a token starts with or ends with one or more punctuations, remove these punctuations, e.g. \"world<font color=\"red\">!!!</font>\" -> \"world\".(<font color=\"blue\">hint, you can import module *string*, use *string.punctuation* to get a list of punctuations (say *puncts*), and then use function *strip(puncts)* to remove leading or trailing punctuations </font>) \n",
    "     * remove the space surrounding each token\n",
    "     * only keep tokens with 2 or more characters, i.e. *len*(token)>1 \n",
    "     * converts all tokens into lower case \n",
    "     * create a dictionary to save the count of each uninque word \n",
    "     * sort the dictionary by word count in descending order\n",
    "     * return the sorted dictionary \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analyzer_q1(text):\n",
    "    cleaned_tokens = []\n",
    "    count_dict = {}\n",
    "\n",
    "    cleaned_tokens = [token.lower() for token in text.split()]\n",
    "    for token in cleaned_tokens:\n",
    "        token = token.strip(string.punctuation)\n",
    "                \n",
    "    for word in cleaned_tokens:\n",
    "        if word not in count_dict:\n",
    "            count_dict[word] = 1\n",
    "        else:\n",
    "            count_dict[word] += 1\n",
    "    \n",
    "    sorted_dict = dict(sorted(count_dict.items(), key=lambda a: a[1], reverse=True))\n",
    "    return sorted_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 7,\n",
       " 'people': 3,\n",
       " 'with': 2,\n",
       " 'moderately': 2,\n",
       " 'severely': 2,\n",
       " 'compromised': 2,\n",
       " 'immune': 2,\n",
       " 'systems': 2,\n",
       " 'are': 2,\n",
       " 'not': 2,\n",
       " 'vaccine': 2,\n",
       " 'additional': 2,\n",
       " 'dose': 2,\n",
       " 'especially': 1,\n",
       " 'vulnerable': 1,\n",
       " 'covid-19,': 1,\n",
       " 'and': 1,\n",
       " 'may': 1,\n",
       " 'build': 1,\n",
       " 'the': 1,\n",
       " 'same': 1,\n",
       " 'level': 1,\n",
       " 'of': 1,\n",
       " 'immunity': 1,\n",
       " '2-dose': 1,\n",
       " 'series': 1,\n",
       " 'compared': 1,\n",
       " 'who': 1,\n",
       " 'immunocompromised.': 1,\n",
       " 'this': 1,\n",
       " 'intended': 1,\n",
       " 'improve': 1,\n",
       " 'immunocompromised': 1,\n",
       " 'people’s': 1,\n",
       " 'response': 1,\n",
       " 'their': 1,\n",
       " 'initial': 1,\n",
       " 'series.': 1,\n",
       " 'cdc': 1,\n",
       " 'recommends': 1,\n",
       " 'that': 1,\n",
       " 'receive': 1,\n",
       " 'an': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "text = '''People with moderately to severely compromised immune systems are especially vulnerable to COVID-19, \n",
    "    and may not build the same level of immunity to 2-dose vaccine series compared to people \n",
    "    who are not immunocompromised. This additional dose intended to improve immunocompromised \n",
    "    people’s response to their initial vaccine series. CDC recommends that people with moderately to \n",
    "    severely compromised immune systems receive an additional dose '''\n",
    "\n",
    "text_analyzer_q1(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Define a function to analyze a numpy array (4 points)\n",
    " - Assume we have an array $X$ which contains term frequency of each document. In this array, each row presents a document, each column denotes a word, and each value, say $x_{i,j}$,  denotes the frequency of the word $j$ in document $i$. Therefore, if there are  $m$ documents, $n$ words, $X$ has a shape of $(m, n)$.\n",
    " \n",
    " Define a function which:\n",
    "      * Take $X$ as an input.\n",
    "      * Divides word frequency $x_{i,j}$ by the total number of words in document $i$. Save the result as an array named $tf$ ($tf$ has shape of $(m,n)$).\n",
    "      * Calculate the document frequency $df_j$ for word $j$, e.g. how many documents contain word $j$. Save the result to array $df$ ($df$ has shape of $(n,)$).\n",
    "      * Calculate $idf_j =  ln(\\frac{|m|}{df_j})+1$. m is the number of documents. The reason is, if a word appears in most documents, it does not have the discriminative power and often is called a `stop` word. The inverse of $df$ can downgrade the weight of such words. ($idf_j$ has shape of $(n,)$).\n",
    "      * For each $x_{i,j}$, calculates $tf\\_idf_{i,j} = tf_(i,j) * idf_j$. ($tf\\_idf$ has shape of $(m,n)$).\n",
    "      * Now, please print the following:\n",
    "          * print the index of the longest document\n",
    "          * print the indexes of words with the top 3 largest $df$ values\n",
    "          * for the longest document, print the indexes of words with top 3 largest values in the $tf\\_idf$ array (use the index you got previously). \n",
    "      * Return the $tf\\_idf$ array.\n",
    " - Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analyzer_q2(X):\n",
    "    X_binarised = np.where(X>0, True, False)\n",
    "    word_count = np.sum(X_binarised, axis=1)\n",
    "\n",
    "    tf = np.divide(X, word_count.reshape(3,1))\n",
    "    \n",
    "    df = np.sum(X_binarised, axis=0)\n",
    "    \n",
    "    idf = np.log(len(X)/df) + 1\n",
    "    \n",
    "    tf_idf = tf * idf\n",
    "   \n",
    "    print(f\"Index of the longest document: {np.argmax(word_count)}\")\n",
    "    \n",
    "    print(f\"Indices of the words with top 3 df values: {df.argsort()[-3:][::-1]}\")\n",
    "    \n",
    "    print(f\"Indices of the words with top 3 tf_idf values in the longest document: {tf_idf[np.argmax(word_count)].argsort()[-3:][::-1]}\")\n",
    "\n",
    "    return tf_idf\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the longest document: 0\n",
      "Indices of the words with top 3 df values: [ 4  5 15]\n",
      "Indices of the words with top 3 tf_idf values in the longest document: [6 3 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.29980176, 0.29980176, 0.29980176, 0.29980176, 0.14285714,\n",
       "        0.20078073, 0.29980176, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.14285714,\n",
       "        0.20078073, 0.        , 0.29980176, 0.29980176, 0.29980176,\n",
       "        0.29980176, 0.29980176, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.41972246, 0.41972246, 0.41972246,\n",
       "        0.41972246]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtm.csv is a csv file for test. \n",
    "# It contains word counts in a few documents\n",
    "dtm = pd.read_csv(\"dtm.csv\")\n",
    "text_analyzer_q2(dtm.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Define a function to analyze a dataset using pandas (3 points)\n",
    "\n",
    "- The dataset \"emotion.csv\" contains a number of text and ten types of sentiment scores. Define a function named `emotion_analysis` to do the follows:\n",
    "   * Read \"emotion.csv\" as a dataframe with the first row in the csv file as column names\n",
    "   * Count the number of samples labeled for each emotion (i.e. each value in the column \"emotion). Print the counts.\n",
    "   * Add a column \"length\" that calculates the number of words for each text. (hint: \"apply\" function to split the text by space and then count elements in the resulting list)\n",
    "   * Show the min, max, and mean values of sadness, happiness, and text length for each emotion. Print the results.\n",
    "   * Create a cross tabulation of average anxiety scores. Use \"emotion\" as row index, \"worry\" as column index, and \"anxiety\" as values. Print the table.\n",
    " - This function does not have any return. Just print out the result of each calculation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_analysis():\n",
    "    pass\n",
    "    # read data\n",
    "    df = pd.read_csv(\"emotion.csv\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"===The number of samples labeled for each emotion===\")\n",
    "    print(df.emotion.value_counts())\n",
    "    \n",
    "    df[\"length\"] = df.loc[:, 'text'].apply(lambda text: len(text.split()))\n",
    "    print(df['length'])\n",
    "    print(\"=== min, max, and mean values of sadness, happiness, and text length for each emotion===\")\n",
    "    # print(pd.pivot_table(index='emotion'))\n",
    "    var = df.groupby([\"emotion\"])[\"sadness\", 'happiness', 'length'].agg([np.amin, np.amax, np.mean])\n",
    "    print(var)\n",
    "    # pd.pivot_table(index = )\n",
    "   \n",
    "    # get cross tab\n",
    "   \n",
    "    # add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   worry     emotion  anger  disgust  fear  anxiety  sadness  happiness  \\\n",
      "0      3     Sadness      5        5     3        7        7          2   \n",
      "1      8     Anxiety      6        7     7        8        6          4   \n",
      "2      4  Relaxation      1        1     2        2        4          7   \n",
      "3      6  Relaxation      4        2     3        4        1          6   \n",
      "4      6     Anxiety      2        2     5        5        5          4   \n",
      "\n",
      "   relaxation  desire                                               text  \n",
      "0           4       5  It is less an much an issue of how it affects ...  \n",
      "1           3       1  I am concerned that the true impact of the cur...  \n",
      "2           7       2  Personally, I am fairly calm about the corona ...  \n",
      "3           7       3  In this very moment as I am fortunate to be ab...  \n",
      "4           4       4  I am more worried about getting access to my n...  \n",
      "===The number of samples labeled for each emotion===\n",
      "Anxiety       1381\n",
      "Sadness        357\n",
      "Relaxation     333\n",
      "Fear           230\n",
      "Anger          107\n",
      "Happiness       39\n",
      "Desire          27\n",
      "Disgust         17\n",
      "Name: emotion, dtype: int64\n",
      "0       102\n",
      "1       243\n",
      "2       122\n",
      "3       124\n",
      "4       120\n",
      "       ... \n",
      "2486    103\n",
      "2487    192\n",
      "2488    137\n",
      "2489     98\n",
      "2490    199\n",
      "Name: length, Length: 2491, dtype: int64\n",
      "=== min, max, and mean values of sadness, happiness, and text length for each emotion===\n",
      "           sadness                happiness                length        \\\n",
      "              amin amax      mean      amin amax      mean   amin  amax   \n",
      "emotion                                                                   \n",
      "Anger            1    9  5.672897         1    8  3.177570     85   312   \n",
      "Anxiety          1    9  5.719768         1    9  3.333816     59   540   \n",
      "Desire           1    8  4.148148         2    8  4.925926     88  1016   \n",
      "Disgust          1    8  4.764706         1    6  3.764706     60   158   \n",
      "Fear             1    9  6.565217         1    9  3.056522     80   319   \n",
      "Happiness        1    9  2.666667         4    9  7.230769     92   274   \n",
      "Relaxation       1    9  2.858859         1    9  5.369369      6   297   \n",
      "Sadness          2    9  7.436975         1    9  3.112045     75   547   \n",
      "\n",
      "                        \n",
      "                  mean  \n",
      "emotion                 \n",
      "Anger       118.897196  \n",
      "Anxiety     117.431571  \n",
      "Desire      150.592593  \n",
      "Disgust     108.529412  \n",
      "Fear        118.039130  \n",
      "Happiness   121.358974  \n",
      "Relaxation  117.804805  \n",
      "Sadness     120.375350  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ec6ad83052f6>:14: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  var = df.groupby([\"emotion\"])[\"sadness\", 'happiness', 'length'].agg([np.amin, np.amax, np.mean])\n"
     ]
    }
   ],
   "source": [
    "emotion_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus question (3 points)\n",
    "1. Suppose your machine learning model returns a list of probabilities as the output. Write a function to do the following:\n",
    "    - Given a threshold, say $th$, if a probability > $th$, the prediction is positive; otherwise, negative\n",
    "    - Compare the prediction with the ground truth labels to calculate the confusion matrix as [[TN, FN],[FP,TP]], where:\n",
    "        * True Positives (TP): the number of correct positive predictions\n",
    "        * False Positives (FP): the number of postive predictives which actually are negatives\n",
    "        * True Negatives (TN): the number of correct negative predictions\n",
    "        * False Negatives (FN): the number of negative predictives which actually are positives\n",
    "    - Calculate **precision** as $TP/(TP+FP)$ and **recall** as $TP/(TP+FN)$\n",
    "    - return precision and recall. \n",
    "2. Call this function with $th$ varying from 0.05 to 0.95 with an increase of 0.05. Plot a line chart to see how precision and recall change by $th$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob =np.array([0.28997326, 0.10166073, 0.10759583, 0.0694934 , 0.6767239 ,\n",
    "       0.01446897, 0.15268748, 0.15570522, 0.12159665, 0.22593857,\n",
    "       0.98162019, 0.47418329, 0.09376987, 0.80440782, 0.88361167,\n",
    "       0.21579844, 0.72343069, 0.06605903, 0.15447797, 0.10967575,\n",
    "       0.93020135, 0.06570391, 0.05283854, 0.09668829, 0.05974545,\n",
    "       0.04874688, 0.07562255, 0.11103822, 0.71674525, 0.08507381,\n",
    "       0.630128  , 0.16447478, 0.16914903, 0.1715767 , 0.08040751,\n",
    "       0.7001173 , 0.04428363, 0.19469664, 0.12247959, 0.14000294,\n",
    "       0.02411263, 0.26276603, 0.11377073, 0.07055441, 0.2021157 ,\n",
    "       0.11636899, 0.90348488, 0.10191679, 0.88744523, 0.18938904])\n",
    "\n",
    "truth = np.array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
    "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(prob, truth, th):\n",
    "    conf = [[0, 0], [0, 0]]\n",
    "    \n",
    "    # add your code here\n",
    "\n",
    "   \n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with threhold 0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
