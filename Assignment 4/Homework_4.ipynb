{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW 4: Text preprocessing</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import operator\n",
    "from string import punctuation\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Regular Expression (2 points)\n",
    "\n",
    "Suppose you have scraped the text shown below from an online source. You'd like to extract data using regular expression.\n",
    "\n",
    "Define a **extract** function which:\n",
    "- Takes a piece of text (in the format of shown below) as an input\n",
    "- Extracts data into a list of tuples using regular expression, e.g.  `[('BTC-USD','56,212.15','-58.16','-0.10%'), ('ETH-USD',  ...), ...]`\n",
    "- Returns the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Symbol   Last Price  Change   % Change   Note\\n                  BTC-USD  56,212.15  -58.16   -0.10%   Bitcoin \\n                  ETH-USD  1,787.79  -53.63   -2.91%   Ether\\n                  BNB-USD  1,101,290.51  +5.81   +2.04%   Binance\\n                  USDT-USD  1.0003   -0.0004  -0.04%   Tether\\n                  ADA-USD  1.1187  -0.0528   -4.51%   Cardano\\n      '"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''Symbol   Last Price  Change   % Change   Note\n",
    "                  BTC-USD  56,212.15  -58.16   -0.10%   Bitcoin \n",
    "                  ETH-USD  1,787.79  -53.63   -2.91%   Ether\n",
    "                  BNB-USD  1,101,290.51  +5.81   +2.04%   Binance\n",
    "                  USDT-USD  1.0003   -0.0004  -0.04%   Tether\n",
    "                  ADA-USD  1.1187  -0.0528   -4.51%   Cardano\n",
    "      '''\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "def extract(text):\n",
    "    exp = \"[A-Z]{3,4}-[A-Z]{3}  *\\d{1,2},*\\d{0,3},*\\d{0,3}.*\\d{2,4}%\"\n",
    "    lst = re.findall(exp, text)\n",
    "    final_lst = []\n",
    "    for ele in lst:\n",
    "        split_ele = ele.split(\"  \")\n",
    "        final_lst.append(tuple(split_ele))\n",
    "    return final_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BTC-USD', '56,212.15', '-58.16', ' -0.10%'),\n",
       " ('ETH-USD', '1,787.79', '-53.63', ' -2.91%'),\n",
       " ('BNB-USD', '1,101,290.51', '+5.81', ' +2.04%'),\n",
       " ('USDT-USD', '1.0003', ' -0.0004', '-0.04%'),\n",
       " ('ADA-USD', '1.1187', '-0.0528', ' -4.51%')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "extract(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Collocation (3 points)\n",
    "\n",
    "Define a function `top_collocation(doc, K)` to find top-K collocations in specific patterns in a document as follows:\n",
    "  - Takes a document (i.e. `doc`) and `K` as inputs\n",
    "  - Find collocations as follows:\n",
    "    - Tokenize the document and find POS tag of each token (hint: you can use NLTK word tokenizer or Spacy tokenizer).\n",
    "    - Create bigrams from the tokens with POS tags.\n",
    "\n",
    "    - Keep only bigrams matching the following patterns:\n",
    "       - `Adj + Noun`: e.g. linear function\n",
    "       - `Noun + Noun`: e.g. regression coefficient\n",
    "    - Get frequency of each bigram (hint: you can use nltk.FreqDist)\n",
    "    - Returns top K collocations by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "\n",
    "def top_collocation(doc, K):\n",
    "    raw_tokens = nltk.word_tokenize(doc)\n",
    "    tokens = []\n",
    "    new_punctuation = punctuation + \"’\"\n",
    "    for i in range(len(raw_tokens)):\n",
    "        raw_tokens[i] = raw_tokens[i].strip(new_punctuation)\n",
    "\n",
    "    for token in raw_tokens:\n",
    "        if token != '':\n",
    "            tokens.append(token)\n",
    "            \n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    bigrams=list(nltk.bigrams(pos_tokens))\n",
    "    phrases = []\n",
    "    \n",
    "    for b1,b2 in bigrams:\n",
    "        if (b1[1].startswith(\"JJ\") and b2[1].startswith(\"NN\")) or (b1[1].startswith(\"NN\") and b2[1].startswith(\"NN\")):\n",
    "            phrases.append((b1[0], b2[0]))\n",
    "\n",
    "    freq = nltk.FreqDist(phrases)\n",
    "    result = freq.most_common(K)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('public', 'health'), 14),\n",
       " (('community', 'spread'), 9),\n",
       " (('United', 'States'), 8),\n",
       " (('higher', 'risk'), 4),\n",
       " (('COVID-19', 'illness'), 4),\n",
       " (('elevated', 'risk'), 4),\n",
       " (('new', 'coronavirus'), 3),\n",
       " (('health', 'threat'), 3),\n",
       " (('serious', 'COVID-19'), 3),\n",
       " (('new', 'virus'), 3)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "top_collocation(article, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Question and Answering (QA) System (5 points)\n",
    "\n",
    "Develop a QA system which allow you to search for answers in an article. For example, the file `qa.json` contains a research article. This article can answer a number of questions about COVID-19. You will design a solution to automatically search answers to these questions in this article.\n",
    "\n",
    "`qa.json` is taken from https://github.com/deepset-ai/COVID-QA. This file contains a few questions, and answers to these questions have been located in the article. Let's define a QA system and check if your system can locate the right answers.\n",
    "\n",
    "The following script helps you understand `qa.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Summary 21 MAR 2020,\n",
      "https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/summary.html\n",
      "\n",
      "This is a rapidly evolving situation and CDC will provide updated information and guidance as it becomes \n"
     ]
    }
   ],
   "source": [
    "# Retrieve the article\n",
    "\n",
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "# A long article. Just print the first 200 characters\n",
    "print(article[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What age group has the highest rate of severe outcomes?', 'id': 236, 'answers': [{'text': 'people 85 years and older', 'answer_start': 6117}], 'is_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What age group has the highest rate of severe outcomes?',\n",
       " 'How is COVID-19 spread?',\n",
       " 'How many states in the U.S. have reported cases of COVID-19?',\n",
       " 'When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       " 'What should mildly-ill patients do?',\n",
       " 'What type of virus is SARS-CoV-2?',\n",
       " 'What viruses are similar to the COVID-19 coronavirus?',\n",
       " 'What are the phases of a pandemic?',\n",
       " 'At which phase does the peak of the pandemic occur?',\n",
       " 'People with which medical conditions have a higher rate of severe illness?',\n",
       " 'What kind of test can diagnose COVID-19?',\n",
       " 'In what species did the COVID-19 virus likely originate?',\n",
       " 'What risk factors should be considered in addition to clinical symptoms?']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all the questions and answers\n",
    "qas = data[\"qas\"]\n",
    "\n",
    "# show the first question-answer pair. Note the answer starts at the 6117th character\n",
    "print(qas[0])\n",
    "\n",
    "# get all questions\n",
    "qs = [item[\"question\"] for item in qas]\n",
    "qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, following the instructions below step by step to develop the QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1. Tokenizer\n",
    "\n",
    "Define a function `tokenize(doc)`  as follows:\n",
    "   - Take a piece of text (i.e. variable `doc`) as an input\n",
    "   - Split the input text into unigrams\n",
    "   - Clean up tokens as follows:\n",
    "       - Lemmatize all unigrams\n",
    "       - Remove all stop words\n",
    "       - Remove all punctuations\n",
    "       - Convert all unigrams to the lower case \n",
    "       - remove empty unigrams\n",
    "   - Return the list of unigrams after all the processing. (Hint: you can use spacy package for this task. To test if a token is stop word or punctuation, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    split_doc = doc.split()\n",
    "    new_words = []\n",
    "    new_punctuation = punctuation + \"—\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add(\"seem\")\n",
    "    stop_words.add(\"serious\")\n",
    "    for word in split_doc:\n",
    "        new_word = word.strip(punctuation)\n",
    "        new_words.append(new_word)\n",
    "    final_words = []\n",
    "    for word in new_words:\n",
    "        if word.lower() in stop_words or word in new_punctuation or word == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            final_words.append(word)\n",
    "\n",
    "    final_doc = \" \".join(final_words)\n",
    "    load_it = spacy.load(\"en_core_web_sm\")\n",
    "    load_doc = load_it(final_doc)\n",
    "\n",
    "    lemmatized_words = []\n",
    "    for word in load_doc:\n",
    "        lemmatized_words.append(word.lemma_)\n",
    "        \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', 'people', 'people', 'age', 'severe', 'chronic', 'medical', 'condition', 'like', 'heart', 'disease', 'lung', 'disease', 'diabetes', 'example', 'high', 'risk', 'develop', 'covid-19', 'illness']\n"
     ]
    }
   ],
   "source": [
    "doc = 'Older people and people of all ages with severe chronic medical conditions — \\\n",
    "like heart disease, lung disease and diabetes, \\\n",
    "for example — seem to be at higher risk of developing serious COVID-19 illness.'\n",
    "\n",
    "print(tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2. Compute TF-IDF Matrix\n",
    "\n",
    "Define a function `compute_tfidf(docs)` as follows: \n",
    "\n",
    "- Take `docs`, a list of documents (e.g. a list of questions) as an input\n",
    "- Tokenize each document in `docs` using the `tokenize` function defined in Q3.1. \n",
    "- Calculate tf_idf weights as shown in lecture notes (Hint: you can reuse the last code segment in NLP Lecture Notes (II))\n",
    "- Return a smoothed normalized `tf_idf` array. (The result may differ a little bit depending on the tokenize function and packages you use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def token_count(doc):\n",
    "    count_dict = {token : token.count(token) for token in tokenize(doc)}\n",
    "    return count_dict\n",
    "\n",
    "def compute_tfidf(docs):\n",
    "    docs_tokens = {idx : token_count(doc) for idx, doc in enumerate(docs)}\n",
    "\n",
    "    dtm = pd.DataFrame.from_dict(docs_tokens, orient = \"index\" )\n",
    "    dtm = dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "\n",
    "    tf = dtm.values\n",
    "    doc_len = tf.sum(axis = 1, keepdims = True)\n",
    "    tf = np.divide(tf, doc_len)\n",
    "\n",
    "    df = np.where(tf > 0,1,0)\n",
    "\n",
    "    smoothed_idf = np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf = tf * smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.64, 0.85, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.28, 0.28, 0.28,\n",
       "        0.28, 0.28]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function using three questions\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "compute_tfidf(qs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.3. Put Everything Together\n",
    "\n",
    "Define a function `find_solutions(qs, article)` as follows: \n",
    "\n",
    "- Take two inputs:\n",
    "    - `qs`: a list of questions (i.e. strings)\n",
    "    - `article`: a document which may contain answers to the questions\n",
    "- Segment the article into sentences (i.e. `sents`). You will locate the sentence which can answer a question.\n",
    "- Concatenate the questions (`qs`) and sentences (`sents`) into a single list (i.e. `qs + sents`)\n",
    "- Call the function `compute_tfidf` defined in Q3.2 with `qs + sents` to get a `TF-IDF` matrix. (Note, now `qs` and `sents` are converted to TF-IDF vectors in the same dimension. As a result, you can measure their similarities.) \n",
    "- Split the `TF-IDF` matrix into two sub matrices, one corresponding to `qs` and the other for `sents`. \n",
    "- Next, calculate the pairwise cosine similarity between the `qs` and `sents`. With $m$ questions and $n$ sentences, you should get a $m \\times n$ matrix. (hint: you can `sklearn.metrics.pairwise_distances` to calculate pairwise distances between two matrices)\n",
    "- Finally, the answer to each question is the sentence which has the `maximum similarity` to it. \n",
    "- Print out each question and its matched answer. Check if your QA system is able to find the right answer.(Depending on the packages you use, your answer might be a bit different from mine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "\n",
    "def find_solutions(qs, article):\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "    tokens = tokenizer(article)\n",
    "    sentences = [i.text.strip() for i in tokens.sents]\n",
    "    \n",
    "    tfidf = compute_tfidf(qs+sentences)\n",
    "\n",
    "    qsVector = tfidf[:len(qs)]\n",
    "    sentsVector = tfidf[len(qs):]\n",
    "\n",
    "    dist = 1-pairwise_distances(qsVector, sentsVector, metric='cosine')\n",
    "    solutions = []\n",
    "\n",
    "    for i in range(len(qs)):\n",
    "        sentence_index = np.argmax(dist[i])\n",
    "        solutions.append([\"Question: %s\"%qs[i], \"Answer: %s\"%sentences[sentence_index]])\n",
    "    \n",
    "    return solutions                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Question: What age group has the highest rate of severe outcomes?',\n",
       "  'Answer: Reported illnesses have ranged from very mild (including some with no reported symptoms) to severe, including illness resulting in death.'],\n",
       " ['Question: How is COVID-19 spread?',\n",
       "  'Answer: If you have been in China or another affected area or have been exposed to someone sick with COVID-19 in the last 14 days, you will face some limitations on your movement and activity.'],\n",
       " ['Question: How many states in the U.S. have reported cases of COVID-19?',\n",
       "  'Answer: All 50 states have reported cases of COVID-19 to CDC.'],\n",
       " ['Question: When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       "  'Answer: CDC Recommends\\nEveryone can do their part to help us respond to this emerging public health threat:\\nOn March 16, the White House announced a program called “15 Days to Slow the Spread,”pdf iconexternal icon which is a nationwide effort to slow the spread of COVID-19 through the implementation of social distancing at all levels of society.'],\n",
       " ['Question: What should mildly-ill patients do?',\n",
       "  'Answer: Most people have mild illness and are able to recover at home without medical care.'],\n",
       " ['Question: What type of virus is SARS-CoV-2?',\n",
       "  'Answer: Emergence\\nCOVID-19 is caused by a coronavirus.'],\n",
       " ['Question: What viruses are similar to the COVID-19 coronavirus?',\n",
       "  'Answer: Please follow instructions during this time.'],\n",
       " ['Question: What are the phases of a pandemic?',\n",
       "  'Answer: Older people and people of all ages with severe chronic medical conditions — like heart disease, lung disease and diabetes, for example — seem to be at higher risk of developing serious COVID-19 illness.'],\n",
       " ['Question: At which phase does the peak of the pandemic occur?',\n",
       "  'Answer: As a result, most research and guidance around pandemics is specific to influenza, but the same premises can be applied to the current COVID-19 pandemic.'],\n",
       " ['Question: People with which medical conditions have a higher rate of severe illness?',\n",
       "  'Answer: Severity\\nThe complete clinical picture with regard to COVID-19 is not fully known.'],\n",
       " ['Question: What kind of test can diagnose COVID-19?',\n",
       "  'Answer: CDC also has issued guidance for other settings, including:\\nPreparing for COVID-19: Long-term Care Facilities, Nursing Homes\\nDiscontinuation of Home Isolation for Persons with COVID-19\\nCDC has deployed multidisciplinary teams to support state health departments in case identification, contact tracing, clinical management, and public communications.'],\n",
       " ['Question: In what species did the COVID-19 virus likely originate?',\n",
       "  'Answer: If you have been in China or another affected area or have been exposed to someone sick with COVID-19 in the last 14 days, you will face some limitations on your movement and activity.'],\n",
       " ['Question: What risk factors should be considered in addition to clinical symptoms?',\n",
       "  'Answer: If you are a healthcare provider, use your judgment to determine if a patient has signs and symptoms compatible with COVID-19 and whether the patient should be tested.']]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the system\n",
    "\n",
    "find_solutions(qs, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
